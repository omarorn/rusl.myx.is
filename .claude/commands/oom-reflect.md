---
description: Deep reflection on project state â€” what works, what's missing, what could be better
---

## Phase 1: Gather Evidence with Serena

Do NOT rely on memory or assumptions. Use symbolic tools to get the actual state:

1. **`get_symbols_overview`** on key files â€” map what's actually implemented (not just planned)
2. **`search_for_pattern`** for TODO/FIXME/HACK comments â€” find hidden debt
3. **`find_symbol`** with `include_body=true` on critical paths â€” verify implementation quality
4. Read `completed-tasks.md` â€” understand what's been done and when
5. Read `todo.md` / `tasks.md` â€” understand what's planned vs pending
6. Read `SESSION.md` â€” understand current phase and blockers

## Phase 1.5: Measure â€” LSP & Validation Metrics

Collect hard numbers BEFORE making any judgments:

1. **TypeScript errors**: `npm run typecheck 2>&1 | tail -5` â€” exact error count
2. **TODO/FIXME/HACK count**: `search_for_pattern` with pattern `TODO|FIXME|HACK` â€” hidden debt indicator
3. **Test pass rate**: `npm run test` â€” actual functionality proof
4. **Lint violations**: `npm run lint 2>&1 | tail -5` â€” code quality metric
5. **Build status**: `npm run build` â€” does it even compile?
6. **Unused symbols**: use `find_referencing_symbols` on exported symbols â€” find dead code
7. **Import health**: check for `Cannot find module` errors in typecheck output

Use these metrics to calibrate honest status levels â€” e.g., "Feature marked âœ… COMPLETE but has 3 TypeScript errors" is actually ğŸŸ¡ PARTIAL.

## Phase 2: Cross-Reference Promises vs Reality

Use **multiple agents** to audit in parallel:
- **Explore agent** â€” scan for dead code, unused exports, orphaned files
- **feature-dev:code-reviewer** â€” review code quality with confidence-based filtering
- **feature-dev:code-explorer** â€” trace execution paths of critical features

For each promised feature (from PRD, CLAUDE.md, README, or original spec):
1. **Does it exist?** â€” search with Serena
2. **Does it work?** â€” trace the execution path
3. **Is it complete?** â€” check for stubs, TODOs, partial implementations
4. Apply honest status assessment (see `.claude/rules/task-status.md`):
   - âœ… COMPLETE (100%) â€” fully implemented, tested, verified
   - ğŸŸ¢ WORKING (80-99%) â€” core works, minor polish needed
   - ğŸŸ¡ PARTIAL (40-79%) â€” framework exists, significant features missing
   - ğŸŸ  STARTED (10-39%) â€” basic structure, mostly stubs
   - âš ï¸ NOT STARTED (0-9%) â€” interfaces only, no implementation

## Phase 3: Critic's Report

Produce a structured assessment:

### What Works Well
- List features that are genuinely complete and solid
- Highlight good patterns, clean code, well-tested areas

### What's Missing
- Features promised but not implemented
- Tests that should exist but don't
- Documentation gaps
- MCP/plugin integrations not yet connected

### What Could Be Better
- Code quality issues (without being pedantic)
- Architecture concerns
- Performance bottlenecks
- Security gaps
- UX/accessibility issues

### Honest Status Summary
```markdown
| Feature | Promised | Actual Status | Gap |
|---------|----------|---------------|-----|
| [Feature] | âœ… | ğŸŸ¡ PARTIAL (60%) | [What's missing] |
```

### Recommended Priority
1. ğŸ”´ Fix now â€” blocking or broken
2. ğŸŸ¡ Fix soon â€” degraded experience
3. ğŸŸ¢ Fix later â€” polish and optimization

## Phase 4: Update Tracking

- Add findings to `todo.md` with priorities
- Update `completed-tasks.md` with reflection session details
- Run `/reflect` to capture learnings
- Git commit the reflection report

## Rules
- NEVER inflate status â€” use honest assessment levels
- ALWAYS verify with Serena before claiming something works or doesn't
- ALWAYS include file paths and evidence for each finding
- ALWAYS note the Claude session ID in completed-tasks.md
